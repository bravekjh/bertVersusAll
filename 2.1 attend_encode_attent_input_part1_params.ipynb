{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Setup Logging\n",
    "import logging\n",
    "logging_format = '%(asctime)s - %(levelname)s - %(message)s'\n",
    "logging.basicConfig(level=logging.INFO,filename=None, format=logging_format)\n",
    "log = logging.getLogger(__name__)\n",
    "    \n",
    "from att_emb_enc import preproc as pp\n",
    "from att_emb_enc import utils\n",
    "from att_emb_enc import embed\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Create Input Parameters For Different Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-10 14:54:33,375 - INFO - Creating Inputs and Embedding Matrix Files\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]2019-06-10 14:54:33,459 - INFO - Input Data Loaded From data/processed_inputs/X_Y_word_index_maxfeat10000_maxlen300.pkl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded paragram_300_sl999.txt\n",
      "Loaded glove.840B.300d.txt\n",
      "Loaded wiki-news-300d-1M.vec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-10 14:58:57,829 - INFO - 'pattern' package not found; tag filters are not available for English\n",
      "2019-06-10 14:58:57,836 - INFO - loading projection weights from data/embeddings/GoogleNews-vectors-negative300.bin\n",
      "2019-06-10 14:59:29,064 - INFO - loaded (3000000, 300) matrix from data/embeddings/GoogleNews-vectors-negative300.bin\n",
      "2019-06-10 14:59:29,582 - INFO - Embeddings Saved to data/processed_embeddings/avg_embeddings_maxfeat10000.pkl\n",
      "\n",
      " 50%|█████     | 1/2 [04:56<04:56, 296.20s/it]2019-06-10 14:59:29,648 - INFO - Input Data Loaded From data/processed_inputs/X_Y_word_index_maxfeat20000_maxlen300.pkl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded GoogleNews-vectors-negative300.bin\n",
      "Loaded paragram_300_sl999.txt\n",
      "Loaded glove.840B.300d.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-10 15:03:48,565 - INFO - loading projection weights from data/embeddings/GoogleNews-vectors-negative300.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded wiki-news-300d-1M.vec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-10 15:04:20,281 - INFO - loaded (3000000, 300) matrix from data/embeddings/GoogleNews-vectors-negative300.bin\n",
      "2019-06-10 15:04:20,932 - INFO - Embeddings Saved to data/processed_embeddings/avg_embeddings_maxfeat20000.pkl\n",
      "\n",
      "100%|██████████| 2/2 [09:47<00:00, 294.75s/it]\n",
      "2019-06-10 15:04:20,933 - INFO - All Inputs and Embedding Matrix Files Created\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded GoogleNews-vectors-negative300.bin\n"
     ]
    }
   ],
   "source": [
    "# Input Processing Parameters\n",
    "param_grid = {'max_features': [10000, 20000], 'maxlen': [300]}\n",
    "param_grid = list(ParameterGrid(param_grid))\n",
    "\n",
    "log.info(\"Creating Inputs and Embedding Matrix Files\")\n",
    "for grid in tqdm(param_grid):\n",
    "    # Get X,Y, vocab, Save to Pickle\n",
    "    X, Y, vocabulary = utils.create_or_load_preproc(\n",
    "        grid['max_features'], grid['maxlen'],False)\n",
    "    \n",
    "    # Create Embedding Matrix and Save to Pickle\n",
    "    embed_matrix = utils.create_or_load_embeddings(\n",
    "        vocabulary, grid['max_features'],300)\n",
    "log.info(\"All Inputs and Embedding Matrix Files Created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Prepare Data For Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input Processing Parameters\n",
    "input_dir = 'data/processed_inputs/'\n",
    "input_files = [input_dir + fn for fn in os.listdir(input_dir) if 'X_Y' in fn]\n",
    "param_grid = {'n_hidden': [32, 64], 'input_file': input_files}\n",
    "param_grid = list(ParameterGrid(param_grid))\n",
    "out_file = 'data/results.csv'\n",
    "overwrite_results = False\n",
    "\n",
    "# Overwrite Results if Desired\n",
    "if overwrite_results is True:\n",
    "    out_connection = open(out_file, 'w')\n",
    "    writer = csv.writer(out_connection)\n",
    "    writer.writerow([\n",
    "        'input_f', 'maxlen', 'max_features', 'n_hidden', \n",
    "        'loss_m', 'loss_std','acc_m', 'acc_std'\n",
    "    ])\n",
    "    out_connection.close()\n",
    "\n",
    "# Run Parameters and Aggregate Results\n",
    "for grid in tqdm(param_grid):\n",
    "    if m.check_if_grid_has_ran(grid, out_file) is True:\n",
    "        print('Grid Already Ran, Skipping')\n",
    "    else:\n",
    "        m.unpack_and_test_grid(grid, dropout=False, out_file=out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bulk Process All Articles\n",
    "# prep_bert(maxlen=300)\n",
    "\n",
    "# Split Test Set From All Articles\n",
    "X_train, X_test, y_train, y_test = load_bert(None,0.1)\n",
    "log.info(f\"Testing Shape = {y_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Parameter Grid\n",
    "n_sample_li = [1000, 5000, 10000, 20000, 30000]\n",
    "n_hidden = [64]\n",
    "param_grid = {'n_hidden': n_hidden, 'number_of_texts': n_sample_li}\n",
    "param_grid = list(ParameterGrid(param_grid))\n",
    "\n",
    "n_samples_li = []\n",
    "n_hidden_li = []\n",
    "val_loss_li = []\n",
    "val_acc_li = []\n",
    "val_prec_li = []\n",
    "val_rec_li = []\n",
    "test_loss_li = []\n",
    "test_acc_li = []\n",
    "test_prec_li = []\n",
    "test_rec_li = []\n",
    "for ix, grid in enumerate(param_grid):\n",
    "    \n",
    "    # Get Data for sample size\n",
    "    X = X_train[0:grid['number_of_texts']].copy()\n",
    "    Y = y_train[0:grid['number_of_texts']].copy()\n",
    "    Xtr, Xval, ytr, yval = train_test_split(\n",
    "        X, Y, test_size=0.25, shuffle=False, random_state=42)\n",
    "    \n",
    "    # Train Model\n",
    "    model, history = mod.train_and_return_model(\n",
    "        Xtr, Xval, ytr, yval, grid['n_hidden'])\n",
    "    \n",
    "    # Extract Results\n",
    "    val_loss, val_acc, val_prec, val_rec = utils.extract_validation_results(history)\n",
    "    test_loss, test_acc, test_prec, test_rec = model.evaluate(X_test, y_test, batch_size=1024, verbose=0)\n",
    "    \n",
    "    # Save To Lists\n",
    "    n_samples_li.append(grid['number_of_texts'])\n",
    "    n_hidden_li.append(grid['n_hidden'])\n",
    "    val_loss_li.append(val_loss)\n",
    "    val_acc_li.append(val_acc)\n",
    "    val_prec_li.append(val_prec)\n",
    "    val_rec_li.append(val_rec)\n",
    "    test_loss_li.append(test_loss)\n",
    "    test_acc_li.append(test_acc)\n",
    "    test_prec_li.append(test_prec)\n",
    "    test_rec_li.append(test_rec)\n",
    "    \n",
    "    # Log Results\n",
    "    log.info(f\"Model {ix} - Prec {test_prec} - Rec {test_rec}\")\n",
    "    log.info(f\"Settings - n_samples = {grid['number_of_texts']}, n_hidden = {grid['n_hidden']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "results_df = pd.DataFrame({\n",
    "    'n_samples': n_samples_li,\n",
    "    'n_hidden': n_hidden_li,\n",
    "    'val_loss': val_loss_li,\n",
    "    'val_accuracy': val_acc_li,\n",
    "    'val_precision': val_prec_li,\n",
    "    'val_recall': val_rec_li,\n",
    "    'test_loss': test_loss_li,\n",
    "    'test_accuracy': test_acc_li,\n",
    "    'test_precision': test_prec_li,\n",
    "    'test_recall': test_rec_li\n",
    "})\n",
    "# Append F1 - Score\n",
    "results_df['val_f1'] = results_df.apply(lambda x: utils.calc_f1(x.val_precision, x.val_recall), axis=1)\n",
    "results_df['test_f1'] = results_df.apply(lambda x: utils.calc_f1(x.test_precision, x.test_recall), axis=1)\n",
    "results_df.sort_values('test_loss', ascending=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
