{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Setup Logging\n",
    "import logging\n",
    "logging_format = '%(asctime)s - %(levelname)s - %(message)s'\n",
    "logging.basicConfig(level=logging.INFO,filename=None, format=logging_format)\n",
    "log = logging.getLogger(__name__)\n",
    "    \n",
    "from minimal_bert import preproc as pp\n",
    "from minimal_bert import modelling as mod\n",
    "from minimal_bert import utils\n",
    "from minimal_bert.utils import prepare_and_save_bert_encoded_df as prep_bert\n",
    "from minimal_bert.utils import load_bert_and_prep_for_training as load_bert\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Datetime\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-10 16:56:56,340 - INFO - Training Shape = (3489, 24)\n",
      "2019-06-10 16:56:56,340 - INFO - Testing Shape = (31397, 24)\n"
     ]
    }
   ],
   "source": [
    "# Bulk Process All Articles\n",
    "# prep_bert(maxlen=300)\n",
    "\n",
    "# Split Test Set From All Articles\n",
    "X_train, X_test, y_train, y_test = load_bert(None,0.1)\n",
    "log.info(f\"Training Shape = {y_test.shape}\")\n",
    "log.info(f\"Testing Shape = {y_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Parameter Grid\n",
    "param_grid = {\n",
    "    'n_hidden': [32, 64, 128], \n",
    "    'number_of_texts': [1000, 1875, 3750, 7500, 15000, 30000], \n",
    "    'activation': ['softmax', 'sigmoid']\n",
    "}\n",
    "param_grid = list(ParameterGrid(param_grid))\n",
    "\n",
    "n_samples_li = []\n",
    "n_hidden_li = []\n",
    "activation_li = []\n",
    "training_time_li = []\n",
    "val_loss_li = []\n",
    "val_acc_li = []\n",
    "val_prec_li = []\n",
    "val_rec_li = []\n",
    "test_loss_li = []\n",
    "test_acc_li = []\n",
    "test_prec_li = []\n",
    "test_rec_li = []\n",
    "\n",
    "for ix, grid in enumerate(param_grid):\n",
    "    start = datetime.now().timestamp()\n",
    "    # Get Data for sample size\n",
    "    X = X_train[0:grid['number_of_texts']].copy()\n",
    "    Y = y_train[0:grid['number_of_texts']].copy()\n",
    "    Xtr, Xval, ytr, yval = train_test_split(\n",
    "        X, Y, test_size=0.25, shuffle=False, random_state=42)\n",
    "    \n",
    "    # Train Model\n",
    "    model, history = mod.train_and_return_model(\n",
    "        Xtr, Xval, ytr, yval, grid['n_hidden'], grid['activation'])\n",
    "    \n",
    "    \n",
    "    # Extract Results\n",
    "    val_loss, val_acc, val_prec, val_rec = utils.extract_validation_results(history)\n",
    "    test_loss, test_acc, test_prec, test_rec = model.evaluate(X_test, y_test, batch_size=1024, verbose=0)\n",
    "    end = datetime.now().timestamp()\n",
    "    \n",
    "    # Save To Lists\n",
    "    n_samples_li.append(grid['number_of_texts'])\n",
    "    n_hidden_li.append(grid['n_hidden'])\n",
    "    training_time_li.append(end - start)\n",
    "    activation_li.append(grid['activation'])\n",
    "    val_loss_li.append(val_loss)\n",
    "    val_acc_li.append(val_acc)\n",
    "    val_prec_li.append(val_prec)\n",
    "    val_rec_li.append(val_rec)\n",
    "    test_loss_li.append(test_loss)\n",
    "    test_acc_li.append(test_acc)\n",
    "    test_prec_li.append(test_prec)\n",
    "    test_rec_li.append(test_rec)\n",
    "    \n",
    "    # Log Results\n",
    "    log.info(f\"Model {ix} - Prec {test_prec} - Rec {test_rec}\")\n",
    "    log.info(f\"Settings - n_samples = {grid['number_of_texts']}, n_hidden = {grid['n_hidden']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "results_df = pd.DataFrame({\n",
    "    'n_samples': n_samples_li,\n",
    "    'n_hidden': n_hidden_li,\n",
    "    'activation':activation_li,\n",
    "    'training_time': training_time_li,\n",
    "    'val_loss': val_loss_li,\n",
    "    'val_accuracy': val_acc_li,\n",
    "    'val_precision': val_prec_li,\n",
    "    'val_recall': val_rec_li,\n",
    "    'test_loss': test_loss_li,\n",
    "    'test_accuracy': test_acc_li,\n",
    "    'test_precision': test_prec_li,\n",
    "    'test_recall': test_rec_li,\n",
    "})\n",
    "# Append F1 - Score\n",
    "results_df['val_f1'] = results_df.apply(lambda x: utils.calc_f1(x.val_precision, x.val_recall), axis=1)\n",
    "results_df['test_f1'] = results_df.apply(lambda x: utils.calc_f1(x.test_precision, x.test_recall), axis=1)\n",
    "results_df.sort_values('test_loss', ascending=True).to_csv('data/results_bert.csv', index=False)\n",
    "results_df.sort_values('test_loss', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "6 * 4 * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
